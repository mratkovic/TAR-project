{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "import sklearn\n",
    "import os\n",
    "import nltk\n",
    "%pylab inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from BeautifulSoup import BeautifulSoup\n",
    "from sklearn.preprocessing import scale\n",
    "import re\n",
    "\n",
    "URL_REGEX = re.compile(r'(https?|ftp)://[^\\s]*')\n",
    "REPLY_REGEX = re.compile(r'@username')\n",
    "\n",
    "URL_TAG = 'URL'\n",
    "REPLY_TAG = 'REP'\n",
    "\n",
    "# PREPROCESS XML DOCUMENTS\n",
    "def extract_from_xml(xml):\n",
    "    bs = BeautifulSoup(xml)\n",
    "    # tab na pocetku\n",
    "    return [document.text.rstrip('\\t') for document in bs.findAll('document')]\n",
    "\n",
    "def trim_multiple_repeats(string):\n",
    "    # coooooool-> coool\n",
    "    # use 3 to make distinct from 2 repeats that are common in \n",
    "    # some languages\n",
    "    return re.sub(r'(.)\\1{3,}', r'\\1\\1\\1', string) \n",
    "\n",
    "def remove_special_data(text):\n",
    "    text = URL_REGEX.sub(URL_TAG, text)\n",
    "    text = REPLY_REGEX.sub(REPLY_TAG, text)\n",
    "    text = trim_multiple_repeats(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def preprocess_tweet(line):\n",
    "    return remove_special_data(line).lower()\n",
    "\n",
    "def get_smile_cnt(string):\n",
    "    return len(re.findall(r'(?::|;|:\\'|=)(?:-)?(?:\\)|\\(|D|P|d|p|3)|<3|</3|xd|xD|XD',string))\n",
    "\n",
    "def cnt_long_repeats(string):\n",
    "    return len(re.findall(r'(\\w)\\1{3,}', string))\n",
    "\n",
    "def cnt_replys(string):\n",
    "    return len(re.findall(r'@username', string))\n",
    "\n",
    "def cnt_hashtags(string):\n",
    "    return len(re.findall(r'#(\\w+)', string))\n",
    "\n",
    "def cnt_exclamations(string):\n",
    "    return len(re.findall(r'!+', string))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test regex\n",
    "line= r'http://www.blabla/test/123 test test'\n",
    "print URL_REGEX.sub('URL', line)\n",
    "\n",
    "#\n",
    "print len(re.findall(r'(.)\\1{3,}', \"haaaaaaaaaaeeee\"))\n",
    "\n",
    "print cnt_hashtags(\"#oov #je #test\")\n",
    "\n",
    "print nltk.word_tokenize('Ovo je test. bla')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "print cnt_exclamations('test!!!! bla!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strukture za pohranu dataseta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LABELS = ['userid', 'gender', 'age_group',\n",
    "          'extroverted', 'stable', 'agreeable',\n",
    "          'conscientious', 'open']\n",
    "TYPES = ['string'] * 3 + ['float'] * 5\n",
    "\n",
    "''' User class used to store parsed data'''\n",
    "class User(object):\n",
    "\n",
    "    def __init__(self, line):\n",
    "        self.labels = LABELS\n",
    "\n",
    "        parts = map(str.strip, line.split(FIELDS_DELIMITER))\n",
    "        if len(parts) == 1:\n",
    "            parts = [parts[0]] + [''] * 7\n",
    "            \n",
    "            self.userid = parts[0]\n",
    "            self.gender = None\n",
    "            self.age_group = None\n",
    "            self.extroverted = None\n",
    "            self.stable = None\n",
    "            self.agreeable = None\n",
    "            self.conscientious = None\n",
    "            self.open = None\n",
    "        \n",
    "        else:\n",
    "            self.userid = parts[0]\n",
    "            self.gender = parts[1]\n",
    "            self.age_group = parts[2]\n",
    "\n",
    "            self.extroverted = float(parts[3])\n",
    "            self.stable = float(parts[4])\n",
    "            self.agreeable = float(parts[5])\n",
    "            self.conscientious = float(parts[6])\n",
    "            self.open = float(parts[7])\n",
    "        self.documents = []\n",
    "\n",
    "    def user_details(self):\n",
    "        return [self.userid, self.gender,\n",
    "                self.age_group, self.extroverted, self.stable,\n",
    "                self.agreeable, self.conscientious, self.open]\n",
    "\n",
    "    def user_documents(self):\n",
    "        return self.documents\n",
    "    \n",
    "    def merge_documents(self):\n",
    "        return '\\n'.join(self.documents)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TRUTH_FILE = 'truth.txt'\n",
    "FIELDS_DELIMITER = ':::'\n",
    "LABELS = ['userid', 'gender', 'age_group',\n",
    "          'extroverted', 'stable', 'agreeable',\n",
    "          'conscientious', 'open']\n",
    "\n",
    "'''Dataset wrapper - parses, cleans and stores user data (documents and truth)\n",
    "'''\n",
    "class Dataset(object):\n",
    "\n",
    "    def __init__(self, path):\n",
    "        if not os.path.exists(path) or not os.path.isdir(path):\n",
    "            raise Exception('No such dir ' + path)\n",
    "\n",
    "        self.path = path\n",
    "        self.users = {}\n",
    "        self.X = []\n",
    "        self.y = []\n",
    "        self.load()\n",
    "        self.labels = LABELS\n",
    "\n",
    "    def load(self):\n",
    "        user_files = filter(lambda name: name != TRUTH_FILE, os.listdir(self.path))\n",
    "        truth = os.path.join(self.path, TRUTH_FILE)\n",
    "        assert os.path.isfile(truth)\n",
    "\n",
    "        # load truth\n",
    "        with open(truth, 'r') as f:\n",
    "            for line in f:\n",
    "                user = User(line)\n",
    "                self.users[user.userid] = user\n",
    "\n",
    "        # load texts\n",
    "        for path in user_files:\n",
    "            user = os.path.splitext(path)[0]\n",
    "            path = os.path.join(self.path, path)\n",
    "\n",
    "            with open(path, 'r') as xml:\n",
    "                content = extract_from_xml(xml.read())\n",
    "                if not self.users.has_key(user):\n",
    "                    self.users[user] = User(user)\n",
    "                self.users[user].documents = content\n",
    "                \n",
    "    def simplify_documents(self):\n",
    "        for key, user in self.users.items():\n",
    "            self.users[key].documents = map(preprocess_tweet, self.users[key].documents)\n",
    "    \n",
    "    def store_as_samples(self):\n",
    "        for id, user in self.users.items():\n",
    "            self.X.append(user.merge_documents())\n",
    "            self.y.append(np.array(user.user_details()))\n",
    "             \n",
    "        self.X = np.array(self.X)\n",
    "        self.y = np.array(self.y)\n",
    "        \n",
    "    def get_documents(self):\n",
    "        x = []\n",
    "        for _, user in self.users.items():\n",
    "            x.append(user.merge_documents())\n",
    "        return np.array(x)\n",
    "        \n",
    "    def get_tweet_len_stats(self):\n",
    "        avgs = []\n",
    "        stds = []\n",
    "        \n",
    "        for _, user in self.users.items():\n",
    "            lens = map(len, user.documents)\n",
    "            avgs.append(np.mean(lens))\n",
    "            stds.append(np.std(lens))\n",
    "        return avgs, stds\n",
    "    \n",
    "    def get_word_len_stats(self):\n",
    "        avgs = []\n",
    "        stds = []\n",
    "        \n",
    "        for _, user in self.users.items():\n",
    "            words = []\n",
    "            for doc in user.documents:\n",
    "                words.extend(doc.split())\n",
    "            \n",
    "            lens = map(len, words)\n",
    "            \n",
    "            \n",
    "            avgs.append(np.mean(lens))\n",
    "            stds.append(np.std(lens))\n",
    "            \n",
    "        return avgs, stds\n",
    "        \n",
    "    def get_samples(self, feature='all'):\n",
    "        if feature == 'all':\n",
    "            return self.X, self.y\n",
    "\n",
    "        feature_col = [i for i, lab in enumerate(self.labels) if lab in feature]\n",
    "        if len(feature_col)==0:\n",
    "            raise Exception('Invalid feature %s\\nValid features %s' %\n",
    "                            (feature, ', '.join(self.labels)))\n",
    "\n",
    "        return self.X, np.array(([ ', '.join(i) for i in self.y[:, feature_col]]))        \n",
    "\n",
    "def append_numeric_feature(features, new_feature):\n",
    "    try:\n",
    "        return np.column_stack((features,scale(map(float, new_feature))))    \n",
    "    except Exception as e:\n",
    "        print 'ERROR', str(e)\n",
    "        for f in new_feature:\n",
    "            try:\n",
    "                float(f)\n",
    "            except:\n",
    "                print f\n",
    "                print \"error\"\n",
    "                \n",
    "def append_numeric_list_of_features(features, new_features):\n",
    "    for f in new_features:\n",
    "        features = append_numeric_feature(features, f)\n",
    "    return features\n",
    "                \n",
    "\n",
    "# len, len_std, cim\n",
    "def additional_features(dataset):\n",
    "    documents = dataset.get_documents()\n",
    "    \n",
    "    avg_len, std_len = dataset.get_tweet_len_stats()\n",
    "    word_len, word_std = dataset.get_word_len_stats()\n",
    "    \n",
    "    \n",
    "    smile_cnt = []\n",
    "    exclamations = []\n",
    "    \n",
    "    hashtags = []\n",
    "    mentions = []\n",
    "    \n",
    "    repeats = []\n",
    "    \n",
    "    \n",
    "    for _, user in dataset.users.items():\n",
    "        cnts = map(get_smile_cnt, user.documents)\n",
    "        smile_cnt.append(np.mean(cnts))\n",
    "        \n",
    "        cnts = map(cnt_exclamations, user.documents)\n",
    "        #map(lambda s: s.count('!'), user.documents)\n",
    "        exclamations.append(np.mean(cnts))\n",
    "        \n",
    "\n",
    "        cnts = map(cnt_long_repeats, user.documents)\n",
    "        repeats.append(np.mean(cnts))\n",
    "        \n",
    "        cnts = map(cnt_hashtags, user.documents)\n",
    "        hashtags.append(np.mean(cnts))\n",
    "        \n",
    "        cnts = map(cnt_replys, user.documents)\n",
    "        mentions.append(np.mean(cnts))\n",
    "    \n",
    "    \n",
    "    return [avg_len, std_len, word_len, word_std, smile_cnt, exclamations, hashtags, mentions, repeats]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pomocni razredi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin\n",
    "'''Transformes sparse matrix to dense - ex. for NaiveBayes'''\n",
    "class DenseTransformer(TransformerMixin):\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.todense()\n",
    "\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        self.fit(X, y, **fit_params)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#simple demo\n",
    "\n",
    "dataset_path = './dataset/english'\n",
    "d = Dataset(dataset_path)\n",
    "extra_features = additional_features(d)\n",
    "d.simplify_documents()\n",
    "\n",
    "d.store_as_samples() #transforme data to X, y\n",
    "\n",
    "X, y = d.get_samples('age_group')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import  GaussianNB\n",
    "from sklearn.pipeline import  Pipeline \n",
    "\n",
    "\n",
    "\n",
    "vectorize = TfidfVectorizer(analyzer='char', ngram_range=(3,3))\n",
    "vectorize_pipe = Pipeline([('vectorize', vectorize), ('densen', DenseTransformer())])\n",
    "X_vectorized = vectorize_pipe.fit_transform(X)\n",
    "print X_vectorized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_vectorized = append_numeric_list_of_features(X_vectorized, extra_features)\n",
    "print X_vectorized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svcPipe = Pipeline([('svc', LinearSVC())])\n",
    "bayesPipe = Pipeline([('bayes', GaussianNB())])\n",
    "\n",
    "svcPipe.fit(X_vectorized, y)\n",
    "print 'SVC training score %s' % svcPipe.score(X_vectorized, y)\n",
    "                    \n",
    "\n",
    "bayesPipe.fit(X_vectorized, y)\n",
    "print 'Bayes training score %s' % bayesPipe.score(X_vectorized, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# bellow code taken and adapted from example\n",
    "# @ http://scikit-learn.org/stable/auto_examples/plot_learning_curve.html\n",
    "from sklearn.learning_curve import learning_curve\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None, n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 10)):\n",
    "\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix,precision_recall_curve, mean_squared_error\n",
    "\n",
    "\n",
    "#TODO jos dodati ovaj precision_recall_curve, pokazivali na predavanjima cini se korisno\n",
    "def printScore(y_true,y_pred,average='micro',pos_label=None):\n",
    "    print average+\" scores:\"\n",
    "    print \"\\t P  = %s\" % precision_score(y_true,y_pred,average=average,pos_label=pos_label)\n",
    "    print \"\\t R  = %s\" % recall_score(y_true,y_pred,average=average,pos_label=pos_label)\n",
    "    print \"\\t F1 = %s\" % f1_score(y_true,y_pred,average=average,pos_label=pos_label)\n",
    "        \n",
    "\n",
    "\n",
    "def modelEvaluator(X, y, model, parameters, classifier = True, scoring = None, num_flods = 3,test_size = 0.3,ylim=None,train_sizes_lncurv=np.linspace(.1, 1.0, 10)):\n",
    "   \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "    \n",
    "    grid_cv = GridSearchCV( model, parameters, scoring = scoring, n_jobs = -1, verbose = 1, cv = num_flods)\n",
    "    grid_cv.fit(X_train,y_train)\n",
    "    \n",
    "    print 'Model best_params: %s' % grid_cv.best_params_\n",
    "    estimator = grid_cv.best_estimator_\n",
    "    print 'Model score : %s' % estimator.score(X_test,y_test)\n",
    "    \n",
    "    y_pred = estimator.predict(X_test)\n",
    "        \n",
    "    #TODO trebaju biti binarne, problem onda treba minjati OVO i OVR \n",
    "    #precision, recall, _ = precision_recall_curve(y_test.ravel(), y_pred.ravel())\n",
    "    #plt.plot(precision,recall)\n",
    "    \n",
    "    if classifier == True:\n",
    "            print \"Confusion matrix:\\n %s\" % confusion_matrix(y_test,y_pred)\n",
    "            if len(set(y)) == 2:\n",
    "                printScore(y_test,y_pred,'binary',list(set(y))[0])\n",
    "            else:\n",
    "                printScore(y_test,y_pred,'macro')\n",
    "                printScore(y_test,y_pred) \n",
    "            plot_learning_curve(estimator, \"Test\", X, y,  ylim=ylim, cv=num_flods,train_sizes=train_sizes_lncurv)\n",
    "    else:\n",
    "        print 'RMSE: ', sqrt(mean_squared_error(estimator.predict(X_test), y_test))\n",
    "    return estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset_path = './dataset/english'\n",
    "d = Dataset(dataset_path)\n",
    "extra_features = additional_features(d)\n",
    "d.simplify_documents()\n",
    "d.store_as_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.naive_bayes import  GaussianNB\n",
    "from sklearn.pipeline import  Pipeline \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "vectorize = TfidfVectorizer(analyzer='char', ngram_range=(3,3))\n",
    "vectorize_pipe = Pipeline([('vectorize', vectorize), ('densen', DenseTransformer())])\n",
    "\n",
    "Xvec = vectorize_pipe.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "parameters = { 'C':linspace(1, 10,100)}\n",
    "\n",
    "modelEvaluator(Xvec, y, LinearSVC(), parameters,num_flods=10)\n",
    "#modelEvaluator(Xvec, y, LogisticRegression(), parameters,num_flods=10)\n",
    "\n",
    "parameters = {'kernel':('linear', 'rbf','poly','sigmoid'), 'C':linspace(1, 1000,100)}\n",
    "modelEvaluator(Xvec, y, SVC(), parameters,num_flods=10)\n",
    "\n",
    "#parameters = {}\n",
    "#modelEvaluator(Xvec, y, RandomForestClassifier(), parameters,num_flods=10)\n",
    "#modelEvaluator(Xvec, y, DecisionTreeClassifier(), parameters,num_flods=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "def evaluate_model(sample_name, models, paramters, num_flods=10, scoring='mean_squared_error',classifier=False):\n",
    "    vectorize = TfidfVectorizer(analyzer='char', ngram_range=(3,3))\n",
    "    \n",
    "    dataset_path = './dataset/english'\n",
    "    d = Dataset(dataset_path)\n",
    "    extra_features = additional_features(d)\n",
    "    d.simplify_documents()\n",
    "    d.store_as_samples()\n",
    "    X, y = d.get_samples(sample_name)\n",
    "    \n",
    "    \n",
    "    y = np.array(y, dtype=float)\n",
    "    vectorize = TfidfVectorizer(analyzer='char', ngram_range=(3,3))\n",
    "    vectorize_pipe = Pipeline([('vectorize', vectorize), ('densen', DenseTransformer())])\n",
    "\n",
    "    Xvec = vectorize_pipe.fit_transform(X)\n",
    "    Xvec = append_numeric_list_of_features(Xvec, extra_features)\n",
    "    \n",
    "    \n",
    "    for model, parameter in zip(models,parameters):\n",
    "        modelEvaluator(Xvec, y, model, parameter, num_flods=num_flods,scoring=scoring,classifier=classifier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def split_samples(X, y):\n",
    "    ys = sorted(set(y))\n",
    "    groups = []\n",
    "    \n",
    "    for y_val in ys:\n",
    "        groups.append([i for i, yy in enumerate(y) if yy == y_val])\n",
    "    return groups, ys\n",
    "\n",
    "def print_class_stats(groups, labels):\n",
    "    #[avg_len, std_len, word_len, word_std, smile_cnt, exclamations, hashtags, mentions, repeats]\n",
    "    for id, feature in enumerate(['avg_len', 'std_len', 'word_len',\n",
    "                                  'word_std', 'smile_cnt', 'exclamations',\n",
    "                                  'hashtags', 'mentions', 'repeats']):\n",
    "        print feature\n",
    "        for label, ids in zip(labels, groups):\n",
    "            print \"%s; cnt: %d\" % (label, len(ids)),\n",
    "            extras = [extra_features[id][i] for i in ids]\n",
    "            print '; average: ', np.mean(extras)\n",
    "        print\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset_path = './dataset/spanish'\n",
    "d = Dataset(dataset_path)\n",
    "extra_features = additional_features(d)\n",
    "d.simplify_documents()\n",
    "d.store_as_samples()\n",
    "\n",
    "X, y = d.get_samples('age_group')\n",
    "groups, labels = split_samples(X, y)\n",
    "print_class_stats(groups, labels)\n",
    "\n",
    "X, y = d.get_samples('gender')\n",
    "groups, labels = split_samples(X, y)\n",
    "print_class_stats(groups, labels)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = [LinearRegression(), DecisionTreeRegressor(), RandomForestRegressor(),SVR()]\n",
    "parameters = [{},{},{},{'kernel':('linear', 'rbf','poly','sigmoid'), 'C':linspace(0.01, 10,100)}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluate_model('extroverted', models, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluate_model('stable', models, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "evaluate_model('agreeable', models, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluate_model('conscientious', models, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluate_model('open', models, parameters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
